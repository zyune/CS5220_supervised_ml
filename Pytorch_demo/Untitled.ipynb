{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0dab5ae",
   "metadata": {},
   "source": [
    "Let’s consider a very basic linear equation i.e., y=2x+1. Here, ‘x’ is the independent variable and y is the dependent variable. We’ll use this equation to create a dummy dataset which will be used to train this linear regression model. Following is the code for creating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34982a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee02d7a",
   "metadata": {},
   "source": [
    "Once we have created the dataset, we can start writing the code for our model. First thing will be to define the model architecture. We do that using the following piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aac5928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ed1d1",
   "metadata": {},
   "source": [
    "We defined a class for linear regression, that inherits torch.nn.Module which is the basic Neural Network module containing all the required functions. Our Linear Regression model only contains one simple linear function.\n",
    "\n",
    "Next, we instantiate the model using the following code.\n",
    "\n",
    "After that, we initialize the loss (Mean Squared Error) and optimization (Stochastic Gradient Descent) functions that we’ll use in the training of this model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e42cbd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633d1b9",
   "metadata": {},
   "source": [
    "After completing all the initializations, we can now begin to train our model. Following is the code for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a9679ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.6661e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 0, loss 7.666120654903352e-05\n",
      "tensor(7.5805e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 1, loss 7.580495002912357e-05\n",
      "tensor(7.4959e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 2, loss 7.495857425965369e-05\n",
      "tensor(7.4120e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 3, loss 7.411999104078859e-05\n",
      "tensor(7.3293e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 4, loss 7.329320214921609e-05\n",
      "tensor(7.2474e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 5, loss 7.247376925079152e-05\n",
      "tensor(7.1666e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 6, loss 7.166604336816818e-05\n",
      "tensor(7.0867e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 7, loss 7.086719415383413e-05\n",
      "tensor(7.0074e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 8, loss 7.007380190771073e-05\n",
      "tensor(6.9291e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 9, loss 6.929115625098348e-05\n",
      "tensor(6.8519e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 10, loss 6.851850776001811e-05\n",
      "tensor(6.7753e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 11, loss 6.775254587410018e-05\n",
      "tensor(6.6996e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 12, loss 6.699572986690328e-05\n",
      "tensor(6.6248e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 13, loss 6.624824163736776e-05\n",
      "tensor(6.5508e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 14, loss 6.550754915224388e-05\n",
      "tensor(6.4777e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 15, loss 6.477681745309383e-05\n",
      "tensor(6.4054e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 16, loss 6.405438034562394e-05\n",
      "tensor(6.3339e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 17, loss 6.333865894703194e-05\n",
      "tensor(6.2633e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 18, loss 6.263308750931174e-05\n",
      "tensor(6.1937e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 19, loss 6.19365309830755e-05\n",
      "tensor(6.1241e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 20, loss 6.124094215920195e-05\n",
      "tensor(6.0558e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 21, loss 6.055827543605119e-05\n",
      "tensor(5.9880e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 22, loss 5.988018529023975e-05\n",
      "tensor(5.9213e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 23, loss 5.9213387430645525e-05\n",
      "tensor(5.8552e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 24, loss 5.8551810070639476e-05\n",
      "tensor(5.7897e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 25, loss 5.7897002989193425e-05\n",
      "tensor(5.7251e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 26, loss 5.725060691474937e-05\n",
      "tensor(5.6613e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 27, loss 5.661313480231911e-05\n",
      "tensor(5.5980e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 28, loss 5.5979508033487946e-05\n",
      "tensor(5.5355e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 29, loss 5.535493983188644e-05\n",
      "tensor(5.4736e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 30, loss 5.4735737649025396e-05\n",
      "tensor(5.4124e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 31, loss 5.412444079411216e-05\n",
      "tensor(5.3520e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 32, loss 5.3520161600317806e-05\n",
      "tensor(5.2924e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 33, loss 5.2924147894373164e-05\n",
      "tensor(5.2333e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 34, loss 5.233306728769094e-05\n",
      "tensor(5.1747e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 35, loss 5.174730540602468e-05\n",
      "tensor(5.1168e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 36, loss 5.1168244681321084e-05\n",
      "tensor(5.0599e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 37, loss 5.059897739556618e-05\n",
      "tensor(5.0034e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 38, loss 5.003410478821024e-05\n",
      "tensor(4.9473e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 39, loss 4.9472961109131575e-05\n",
      "tensor(4.8921e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 40, loss 4.892090510111302e-05\n",
      "tensor(4.8376e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 41, loss 4.837552842218429e-05\n",
      "tensor(4.7838e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 42, loss 4.7837897000135854e-05\n",
      "tensor(4.7303e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 43, loss 4.730272485176101e-05\n",
      "tensor(4.6771e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 44, loss 4.677069227909669e-05\n",
      "tensor(4.6251e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 45, loss 4.625127257895656e-05\n",
      "tensor(4.5733e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 46, loss 4.573308251565322e-05\n",
      "tensor(4.5221e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 47, loss 4.522063318290748e-05\n",
      "tensor(4.4715e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 48, loss 4.4714644900523126e-05\n",
      "tensor(4.4217e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 49, loss 4.421654011821374e-05\n",
      "tensor(4.3726e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 50, loss 4.3725860450649634e-05\n",
      "tensor(4.3237e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 51, loss 4.3237207137281075e-05\n",
      "tensor(4.2754e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 52, loss 4.2754421883728355e-05\n",
      "tensor(4.2275e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 53, loss 4.227495810482651e-05\n",
      "tensor(4.1805e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 54, loss 4.1804549255175516e-05\n",
      "tensor(4.1336e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 55, loss 4.133630500291474e-05\n",
      "tensor(4.0877e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 56, loss 4.087744309799746e-05\n",
      "tensor(4.0419e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 57, loss 4.041868669446558e-05\n",
      "tensor(3.9967e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 58, loss 3.996697341790423e-05\n",
      "tensor(3.9519e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 59, loss 3.951899998355657e-05\n",
      "tensor(3.9080e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 60, loss 3.908044527634047e-05\n",
      "tensor(3.8644e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 61, loss 3.864417885779403e-05\n",
      "tensor(3.8210e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 62, loss 3.820992060354911e-05\n",
      "tensor(3.7784e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 63, loss 3.7784218875458464e-05\n",
      "tensor(3.7364e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 64, loss 3.736395956366323e-05\n",
      "tensor(3.6946e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 65, loss 3.694592305691913e-05\n",
      "tensor(3.6534e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 66, loss 3.653420935734175e-05\n",
      "tensor(3.6125e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 67, loss 3.612522050389089e-05\n",
      "tensor(3.5723e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 68, loss 3.572334026102908e-05\n",
      "tensor(3.5324e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 69, loss 3.5324410418979824e-05\n",
      "tensor(3.4931e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 70, loss 3.493108306429349e-05\n",
      "tensor(3.4541e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 71, loss 3.454068428254686e-05\n",
      "tensor(3.4154e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 72, loss 3.4154127206420526e-05\n",
      "tensor(3.3771e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 73, loss 3.377095708856359e-05\n",
      "tensor(3.3394e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 74, loss 3.3394131605746225e-05\n",
      "tensor(3.3022e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 75, loss 3.3021675335476175e-05\n",
      "tensor(3.2654e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 76, loss 3.265426494181156e-05\n",
      "tensor(3.2289e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 77, loss 3.2288997317664325e-05\n",
      "tensor(3.1928e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 78, loss 3.1928400858305395e-05\n",
      "tensor(3.1572e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 79, loss 3.1571606086799875e-05\n",
      "tensor(3.1218e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 80, loss 3.121849294984713e-05\n",
      "tensor(3.0873e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 81, loss 3.087255754508078e-05\n",
      "tensor(3.0526e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 82, loss 3.052575993933715e-05\n",
      "tensor(3.0187e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 83, loss 3.018688403244596e-05\n",
      "tensor(2.9848e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 84, loss 2.9848073609173298e-05\n",
      "tensor(2.9515e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 85, loss 2.9514649213524535e-05\n",
      "tensor(2.9187e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 86, loss 2.9186885512899607e-05\n",
      "tensor(2.8859e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 87, loss 2.8859269150416367e-05\n",
      "tensor(2.8537e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 88, loss 2.853652222256642e-05\n",
      "tensor(2.8218e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 89, loss 2.82177406916162e-05\n",
      "tensor(2.7902e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 90, loss 2.790247162920423e-05\n",
      "tensor(2.7591e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 91, loss 2.7590756872086786e-05\n",
      "tensor(2.7284e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 92, loss 2.7283684175927192e-05\n",
      "tensor(2.6978e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 93, loss 2.6978093956131488e-05\n",
      "tensor(2.6677e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 94, loss 2.6676634661271237e-05\n",
      "tensor(2.6380e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 95, loss 2.6379888367955573e-05\n",
      "tensor(2.6085e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 96, loss 2.608506656542886e-05\n",
      "tensor(2.5793e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 97, loss 2.5793300665100105e-05\n",
      "tensor(2.5505e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 98, loss 2.5505403755232692e-05\n",
      "tensor(2.5221e-05, grad_fn=<MseLossBackward0>)\n",
      "epoch 99, loss 2.522112481528893e-05\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69b505",
   "metadata": {},
   "source": [
    "Now that our Linear Regression Model is trained, let’s test it. Since it’s a very trivial model, we’ll test this on our existing dataset and also plot to see the original vs the predicted outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c7d50c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.009342 ]\n",
      " [ 3.0079966]\n",
      " [ 5.0066514]\n",
      " [ 7.0053062]\n",
      " [ 9.003961 ]\n",
      " [11.002616 ]\n",
      " [13.00127  ]\n",
      " [14.999926 ]\n",
      " [16.998579 ]\n",
      " [18.997234 ]\n",
      " [20.99589  ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d4ac4ca1e82caf0dfdb4c1dde3cd7d61471ca19c5e6430ce805f885f7c689ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
